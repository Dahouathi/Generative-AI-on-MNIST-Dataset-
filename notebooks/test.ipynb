{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f19a7d-7185-4932-9fe0-dbd9cd09e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')  # Notebook is in the 'notebooks' directory\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from src.data_utils import lire_alpha_digit\n",
    "from src.dbn import DBN\n",
    "from src.rbm import RBM\n",
    "from src.data_utils import lire_alpha_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94c904af-609d-49dd-9de6-44c7897de2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "def sigmoid_prime(z):\n",
    "    return z * (1 - z)\n",
    "\n",
    "\n",
    "def calcul_softmax(rbm: RBM, data):\n",
    "    z = np.array(rbm.b) + np.dot(data, rbm.W)\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "class DNN:\n",
    "    def __init__(self, config, output_dim=10):\n",
    "        \"\"\"\n",
    "        :param config: configuration of the DBN\n",
    "        :param output_dim: dimension of the output\n",
    "        \"\"\"\n",
    "        self.config: tuple = config\n",
    "        self.num_layers: int = len(self.config)  # number of layers except output\n",
    "\n",
    "        # un DNN est un DBN avec une couche de classification supplémentaire\n",
    "        # dernier RBM du DBN pour la classification → on ne définit pas \"rbm.a\".\n",
    "        self.dbn: DBN = DBN(config)\n",
    "        self.classification: RBM = RBM(config[-1], output_dim)\n",
    "        self.pretrained: bool = False  # check if model is pretrained\n",
    "        self.fitted: bool = False  # check if model is fitted\n",
    "\n",
    "    def pretrain_dnn(self, data, epochs=100, learning_rate=0.1, batch_size=100):\n",
    "        self.dbn.train_dbn(data, epochs, learning_rate, batch_size)\n",
    "        self.pretrained = True\n",
    "        return self\n",
    "\n",
    "    def entree_sortie_network(self, data):\n",
    "        v = data.copy()\n",
    "        results = [v]  # Couche d'entrée\n",
    "        for i in range(self.num_layers - 1):\n",
    "            p_h = self.dbn.dbn[i].entree_sortie_rbm(v)\n",
    "            v = np.random.binomial(1, p_h)\n",
    "            results.append(p_h)\n",
    "\n",
    "        # Compute the probabilities\n",
    "        softmax_probas = calcul_softmax(self.classification, v)\n",
    "        results.append(softmax_probas)\n",
    "        return results\n",
    "\n",
    "    def backward_propagation(\n",
    "        self,\n",
    "        data,\n",
    "        labels,\n",
    "        epochs=100,\n",
    "        learning_rate=0.1,\n",
    "        batch_size=100,\n",
    "        early_stopping=5,\n",
    "        verbose=True,\n",
    "        plot=True,\n",
    "    ):\n",
    "        keep_track = 0\n",
    "        train_loss = 100\n",
    "        loss_batches, loss = [], []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            data_copy = data.copy()\n",
    "            labels_copy = pd.get_dummies(labels.copy())\n",
    "            data_copy, labels_copy = shuffle(data_copy, labels_copy)\n",
    "\n",
    "            for batch in range(0, data.shape[0], batch_size):\n",
    "                data_batch = data_copy[\n",
    "                    batch: min(batch + batch_size, data.shape[0]), :\n",
    "                ]\n",
    "                labels_batch = labels_copy[\n",
    "                    batch: min(batch + batch_size, data.shape[0])\n",
    "                ]\n",
    "                # Forward pass\n",
    "                activations = self.entree_sortie_network(data_batch)\n",
    "\n",
    "                # Loss\n",
    "                loss_batches.append(\n",
    "                    -np.mean(np.sum(labels_batch * np.log(activations[-1]), axis=1))\n",
    "                )\n",
    "\n",
    "                # Backward pass\n",
    "                # Start with last layer\n",
    "                delta = activations[-1] - labels_batch\n",
    "                grad_w = np.dot(activations[-2].T, delta) / batch_size\n",
    "                grad_b = np.mean(delta, axis=0)\n",
    "                self.classification.W -= learning_rate * grad_w\n",
    "                self.classification.b -= learning_rate * grad_b\n",
    "\n",
    "                # Propagate error backwards through hidden layers\n",
    "                for layer in range(1, self.num_layers):\n",
    "                    if layer == 1:\n",
    "                        delta = np.dot(delta, self.classification.W.T) * sigmoid_prime(\n",
    "                            activations[-layer - 1]\n",
    "                        )\n",
    "                    else:\n",
    "                        delta = np.dot(\n",
    "                            delta, self.dbn.dbn[-layer + 1].W.T\n",
    "                        ) * sigmoid_prime(activations[-layer - 1])\n",
    "                    if layer == self.num_layers - 1:\n",
    "                        grad_w = np.dot(data_batch.T, delta) / batch_size\n",
    "                    else:\n",
    "                        grad_w = np.dot(activations[-layer - 2].T, delta) / batch_size\n",
    "                    grad_b = np.mean(delta, axis=0)\n",
    "                    self.dbn.dbn[-layer].W -= learning_rate * grad_w\n",
    "                    self.dbn.dbn[-layer].b -= learning_rate * grad_b\n",
    "\n",
    "            # Compute cross-entropy loss\n",
    "            previous_loss = train_loss\n",
    "            train_loss = float(np.mean(loss_batches))\n",
    "            loss.append(train_loss)\n",
    "\n",
    "            if keep_track < early_stopping and round(train_loss, 3) == round(previous_loss, 3):\n",
    "                keep_track += 1\n",
    "            elif keep_track == early_stopping:\n",
    "                return self\n",
    "            # Print progress\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Epoch {epoch}/{epochs}: Train error -----------------{train_loss:.4f}\"\n",
    "                )\n",
    "        if plot:\n",
    "            plt.plot(np.arange(epochs), loss)\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"CrossEntropy Loss\")\n",
    "            if self.pretrained:\n",
    "                plt.title(\"Loss for pretrained DNN\")\n",
    "            else:\n",
    "                plt.title(\"Loss for DNN (without pretraining)\")\n",
    "            plt.show()\n",
    "\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def test_dnn(self, test_data, test_labels, verbose=True):\n",
    "        probs = self.entree_sortie_network(test_data)\n",
    "        pred_label = np.argmax(probs[-1], axis=1)\n",
    "        num_correct = np.sum(test_labels != pred_label)\n",
    "\n",
    "        # Print the error rate and return it\n",
    "        error_rate = num_correct / test_data.shape[0]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Error rate ----------------- : {error_rate:.2%}\")\n",
    "        return error_rate\n",
    "\n",
    "    def plot_proba(self, data):\n",
    "        pred_labels = self.entree_sortie_network(data)[-1]\n",
    "        plt.scatter(np.arange(0, 10), pred_labels[0])\n",
    "        plt.xlabel(\"Classes\")\n",
    "        plt.ylabel(\"Predicted probability for each class\")\n",
    "        plt.title(\"Probabilities by class\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d11cc04-5f91-4a94-be9e-dbf9a0fd0a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((195, 320), (195,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "data_path = '../data/binary_alpha_digits/binaryalphadigs.mat'\n",
    "X = lire_alpha_digit(data_path, np.arange(5))\n",
    "y_labels = []\n",
    "for i in range(5):\n",
    "    y_labels.extend([i] * (X.shape[0] // 5))\n",
    "y = np.array(y_labels)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "febc9c61-6720-4b26-8eda-346190c3ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = DNN([320, 200, 200],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c2e3550-d83c-4707-aae8-7be51e45cfe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DBN' object has no attribute 'dbn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 76\u001b[0m, in \u001b[0;36mDNN.backward_propagation\u001b[0;34m(self, data, labels, epochs, learning_rate, batch_size, early_stopping, verbose, plot)\u001b[0m\n\u001b[1;32m     72\u001b[0m labels_batch \u001b[38;5;241m=\u001b[39m labels_copy[\n\u001b[1;32m     73\u001b[0m     batch: \u001b[38;5;28mmin\u001b[39m(batch \u001b[38;5;241m+\u001b[39m batch_size, data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     74\u001b[0m ]\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentree_sortie_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Loss\u001b[39;00m\n\u001b[1;32m     79\u001b[0m loss_batches\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39msum(labels_batch \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     81\u001b[0m )\n",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m, in \u001b[0;36mDNN.entree_sortie_network\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     37\u001b[0m results \u001b[38;5;241m=\u001b[39m [v]  \u001b[38;5;66;03m# Couche d'entrée\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 39\u001b[0m     p_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdbn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdbn\u001b[49m[i]\u001b[38;5;241m.\u001b[39mentree_sortie_rbm(v)\n\u001b[1;32m     40\u001b[0m     v \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mbinomial(\u001b[38;5;241m1\u001b[39m, p_h)\n\u001b[1;32m     41\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(p_h)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DBN' object has no attribute 'dbn'"
     ]
    }
   ],
   "source": [
    "dnn.backward_propagation(X, y, epochs=100, learning_rate=0.1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e0ee5-013a-4249-8845-4fe9e0504866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
