{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction and Test of DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')  # Notebook is in the 'notebooks' directory\n",
    "from src.dbn import init_DBN, train_DBN\n",
    "from src.rbm import entree_sortie_rbm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from src.data_utils import lire_alpha_digit\n",
    "from src.dbn import DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_DNN(sizes, output_size=10):\n",
    "    \"\"\"\n",
    "    Initialize a Deep Neural Network (DNN) with the given sizes and output size.\n",
    "    param: sizes: list of integers representing the number of nodes in each layer\n",
    "    param: output_size: number of nodes in the output layer\n",
    "    return: the initialized DNN model\n",
    "    \"\"\"\n",
    "    configuration = sizes + [output_size]\n",
    "    print(configuration)\n",
    "    return init_DBN(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((195, 320), (195,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "data_path = '../data/binary_alpha_digits/binaryalphadigs.mat'\n",
    "X = lire_alpha_digit(data_path, np.arange(5))\n",
    "y_labels = []\n",
    "for i in range(5):\n",
    "    y_labels.extend([i] * (X.shape[0] // 5))\n",
    "y = np.array(y_labels)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[320, 200, 200, 5]\n",
      "3 (320, 200)\n"
     ]
    }
   ],
   "source": [
    "dnn = init_DNN([320, 200, 200],5)\n",
    "print(len(dnn), dnn[0]['W'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_DNN(X, dnn, epochs=100, learning_rate=0.1, batch_size=128):\n",
    "    \"\"\"\n",
    "    Pretrain a Deep Neural Network (DNN) using the given data.\n",
    "    param: X: training data\n",
    "    param: dnn: the DNN model\n",
    "    param: epochs: number of training epochs\n",
    "    param: learning_rate: learning rate\n",
    "    param: batch_size: size of mini-batches\n",
    "    return: the pretrained DNN model\n",
    "    \"\"\"\n",
    "    dbn = dnn[:-1]\n",
    "    dbn = train_DBN(X, dbn, epochs=epochs, learning_rate=learning_rate, batch_size=batch_size)\n",
    "    dnn[:-1] = dbn\n",
    "    return dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer 1\n",
      "Training layer 2\n"
     ]
    }
   ],
   "source": [
    "dnn = pretrain_DNN(X, dnn, epochs=100, learning_rate=0.1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_softmax(rbm, X):\n",
    "    \"\"\"\n",
    "    Calculate the softmax probabilities for the given layer and input data.\n",
    "    param: layer: a dictionary containing the weights 'W' and biases 'b' of the layer\n",
    "    param: X: input data\n",
    "    return: softmax probabilities\n",
    "    \"\"\"\n",
    "    # Calculate the logits\n",
    "    logits = np.dot(X, rbm['W']) + rbm['b']\n",
    "    \n",
    "    # Apply the softmax function\n",
    "    exp_logits = np.exp(logits)\n",
    "    softmax_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    \n",
    "    return softmax_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entree_sortie_reseau(DNN, X):\n",
    "    \"\"\"\n",
    "    Compute the output of the Deep Neural Network (DNN) given the input.\n",
    "    param: DNN: the DNN model\n",
    "    param: X: the input data\n",
    "    return: the output of the DNN\n",
    "    \"\"\"\n",
    "    sorties = [X]  # List to store the outputs of each layer\n",
    "    proba_sortie = None  # Will store the output of the last layer\n",
    "    \n",
    "    for i in range(len(DNN)-1):\n",
    "        rbm = DNN[i]\n",
    "        v = np.random.binomial(1, entree_sortie_rbm(rbm, sorties[-1]))\n",
    "        sorties.append(v)\n",
    "    rbm_classification = DNN[-1]\n",
    "    proba_sortie = calcul_softmax(rbm_classification, sorties[-1])\n",
    "    \n",
    "    return sorties, proba_sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorties, proba_sortie = entree_sortie_reseau(dnn, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((195, 320), (195, 5))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorties[0].shape, proba_sortie.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "def retropropagation(X, y, dnn, epochs=100, learning_rate=0.1, batch_size=128, verbose=True, plot=True, pretrained=False, save_path=None):\n",
    "    \"\"\"\n",
    "    Train a Deep Neural Network (DNN) using the given data.\n",
    "    param: X: training data\n",
    "    param: y: target labels\n",
    "    param: dnn: the DNN model\n",
    "    param: pretrained: whether the DNN is pretrained\n",
    "    param: epochs: number of training epochs\n",
    "    param: learning_rate: learning rate\n",
    "    param: batch_size: size of mini-batches\n",
    "    param: verbose: whether to print the loss at each epoch\n",
    "    param: plot: whether to plot the loss\n",
    "    return: the trained DNN model\"\"\"\n",
    "    # Convert y to one-hot encoded format using pd.get_dummies\n",
    "    y_one_hot = pd.get_dummies(y).values\n",
    "\n",
    "    best_loss = float('inf')  # Initialize the minimum loss to infinity\n",
    "    loss = []\n",
    "    patience = 100  # Number of epochs to wait before early stopping\n",
    "    wait = 0\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the data using sklearn shuffle\n",
    "        X_shuffled, y_shuffled = shuffle(X, y_one_hot)\n",
    "        \n",
    "        loss_batches = []\n",
    "        # Iterate over each mini-batch\n",
    "        for batch in range(0, X.shape[0], batch_size):\n",
    "            \n",
    "            X_batch = X_shuffled[batch: min(batch + batch_size, X.shape[0]), :]\n",
    "            y_batch = y_shuffled[batch: min(batch + batch_size, X.shape[0])]\n",
    "\n",
    "            # Forward propagation\n",
    "            sorties, proba_sortie = entree_sortie_reseau(dnn, X_batch)\n",
    "            \n",
    "\n",
    "            # Loss calculation\n",
    "            loss_batch = -np.mean(np.sum(y_batch * np.log(proba_sortie), axis=1))\n",
    "            loss_batches.append(loss_batch)\n",
    "            \n",
    "            # Backward propagation\n",
    "            # Last layer\n",
    "            delta = proba_sortie - y_batch\n",
    "            grad_W = np.dot(sorties[-1].T, delta) / batch_size\n",
    "            grad_b = np.mean(delta, axis=0)\n",
    "            dnn[-1]['W'] -= learning_rate * grad_W\n",
    "            dnn[-1]['b'] -= learning_rate * grad_b\n",
    "\n",
    "            # Hidden layers\n",
    "            for i in range(2, len(dnn)+1):\n",
    "                if i == 2:  # Classification layer\n",
    "                    RBM = dnn[-1]\n",
    "                    delta = np.dot(delta, RBM['W'].T) * sorties[-1] * (1 - sorties[-1])\n",
    "                else:\n",
    "                    RBM = dnn[-i+1]\n",
    "                    delta = np.dot(delta, RBM['W'].T) * sorties[-i+1] * (1 - sorties[-i+1])\n",
    "                if i == len(dnn):\n",
    "                    grad_W = np.dot(X_batch.T, delta)\n",
    "                else:\n",
    "                    grad_W = np.dot(sorties[-i].T, delta)\n",
    "                \n",
    "                grad_b = np.mean(delta, axis=0)\n",
    "    \n",
    "                # Update weights and biases\n",
    "                dnn[-i - 1]['W'] -= learning_rate * grad_W\n",
    "                dnn[-i - 1]['b'] -= learning_rate * grad_b\n",
    "\n",
    "\n",
    "        # Calculate the cross entropy loss for the epoch\n",
    "        train_loss = float(np.mean(loss_batches))\n",
    "        loss.append(train_loss)\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss}\")\n",
    "\n",
    "        # Check if current loss is less than the best loss encountered so far\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            wait = 0  # reset wait since we've seen improvement\n",
    "        else:\n",
    "            wait += 1  # increment wait since there was no improvement\n",
    "\n",
    "        # If we have waited for 'patience' epochs without improvement, stop training\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping due to no improvement in Loss.\")\n",
    "            break\n",
    "        \n",
    "          \n",
    "    \n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(loss), loss)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"CrossEntropy Loss\")\n",
    "        if pretrained:\n",
    "            plt.title(\"Loss for pretrained DNN\")\n",
    "        else:\n",
    "            plt.title(\"Loss for DNN (without pretraining)\")\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show(block=False)\n",
    "        plt.close()\n",
    "    return dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (32,200) (32,320) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dnn \u001b[38;5;241m=\u001b[39m \u001b[43mretropropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 58\u001b[0m, in \u001b[0;36mretropropagation\u001b[0;34m(X, y, dnn, epochs, learning_rate, batch_size, verbose, plot, pretrained, save_path)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     RBM \u001b[38;5;241m=\u001b[39m dnn[\u001b[38;5;241m-\u001b[39mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 58\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRBM\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msorties\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m sorties[\u001b[38;5;241m-\u001b[39mi])\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(dnn) :\n\u001b[1;32m     60\u001b[0m     grad_W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X_batch\u001b[38;5;241m.\u001b[39mT, delta)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (32,200) (32,320) "
     ]
    }
   ],
   "source": [
    "dnn = retropropagation(X, y, dnn, epochs=100, learning_rate=0.1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dnn(X, y, dnn, verbose=False):\n",
    "    _, proba_sortie = entree_sortie_reseau(dnn, X)\n",
    "    predictions = np.argmax(proba_sortie, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    if verbose :\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot_proba(data, dnn, k, save_path=None):\n",
    "    _, proba_sortie = entree_sortie_reseau(dnn, data)\n",
    "    plt.figure()\n",
    "    sns.boxplot(data=proba_sortie)\n",
    "    plt.xlabel(\"Classes\")\n",
    "    plt.ylabel(\"Predicted probability\")\n",
    "    plt.title(\"Distribution of probabilities of the class {k}\")\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show(block=False)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLenv] *",
   "language": "python",
   "name": "conda-env-MLenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
